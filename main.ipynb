{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "#CSV \n",
    "import csv\n",
    "\n",
    "# Excel Sheets\n",
    "import csv\n",
    "\n",
    "# ###########\n",
    "#   Modules - Classes that contain functions used within code\n",
    "# #############\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "# Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "# # Login Module ( For Using Login Class )\n",
    "# from modules.login import Login as login\n",
    "# # Go To Homepage Module ( Making Company Search Query and Navigating to Homepage )\n",
    "# from modules.go_to_homepage import GoToHomepage\n",
    "# # Get Search Module \n",
    "# from modules.get_search import GetSearch\n",
    "# # Get Search Module \n",
    "# from modules.collect import Collect\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "import configparser\n",
    "import pathlib\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "EMAIL = config[\"DEFAULT\"][\"USERNAME\"]\n",
    "KEY = config[\"DEFAULT\"][\"KEY\"]\n",
    "\n",
    "driver = Service('driver/chromedriver')\n",
    "browser = webdriver.Chrome(service=driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Companies and Queries (Search Terms)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET COMPANIES\n",
    "companies = []\n",
    "file_path = \"company_urls.txt\"\n",
    "with open(file_path, 'r') as companies_list:\n",
    "    for i in companies_list.readlines():\n",
    "        name= i.replace('\\n', \"\")\n",
    "        companies.append(name)\n",
    "\n",
    "\n",
    "# SEARCH TERMS\n",
<<<<<<< HEAD
    "search_terms = ['title 1', 'title 2', \"title 3\"]"
=======
    "search_terms = ['title 1', 'title 2', 'title 3']"
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set headers for Exported CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Excel Header Row\n",
<<<<<<< HEAD
    "headers = [\"Company\", \"Title 1 Results\", \"Title 2 Results\", \"Title 3 Results\"]\n",
=======
    "headers = [\"Company URL\", \"Title 1 Results\", \"Title 2 Results\", \"Title 3 Results\"]\n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "\n",
    "with open('final_list.csv', 'a') as f:\n",
    "    object = csv.writer(f)\n",
    "    object.writerow(headers)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login\n",
    "def login(username, key):\n",
    "    browser.get('https://www.linkedin.com/uas/login')\n",
    "    elementID = browser.find_element(By.ID, 'username')\n",
    "    elementID.send_keys(username)\n",
    "    elementID = browser.find_element(By.ID, 'password')\n",
    "    elementID.send_keys(key)\n",
    "    elementID.submit()   \n",
    "\n",
    "login(EMAIL, KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search():\n",
    "\n",
    "    def __init__(self, browser, search_terms, time):\n",
    "        self.browser = browser\n",
    "        self.search_terms = search_terms\n",
    "        self.time = time\n",
    "\n",
    "\n",
    "    # Go To Homepage Method\n",
<<<<<<< HEAD
    "    def go_to_page(self, company):\n",
    "        try:\n",
    "            #  Every company search on linkedin will contain this url syntax or prefix\n",
    "            linkedin_search_baseurl = \"https://www.linkedin.com/search/results/companies/?keywords=\"\n",
    "\n",
    "            # linkedin url's have a dynamic syntax, therefore in order to search for companies whose name contains more than one word, we will need to concatentate the individual words with linkedin's uniuqe syntax ('%20' is placed between each word)\n",
    "\n",
    "            # split the company name so each word is given an index\n",
    "            company_word_list = company.split()\n",
    "\n",
    "            # we need to create a variable that will hold the dynamic suffix to our base linkedin company search url.\n",
    "            url_ending = \"\"\n",
    "\n",
    "            # if the company name is just one word, then the url_ending will simply be the name of the company\n",
    "            if len(company_word_list) == 1:\n",
    "                url_ending += company_word_list[0]\n",
    "\n",
    "            # if the company name is longer than 1 word, the search url syntax will change - For this, we can run a loop over each word in the company name, and if the current word is not first, place '$20' before the word at that current word's index.\n",
    "            else:\n",
    "                url_ending = company_word_list[0]\n",
    "                i = 1\n",
    "                while i < len(company_word_list):\n",
    "                    # each time the loop is ran, a value with be added to url_ending variable we've created\n",
    "                    url_ending += \"%20\" + company_word_list[i]\n",
    "                    i += 1\n",
    "\n",
    "            # Company Insights URL\n",
    "            company_url = linkedin_search_baseurl + url_ending\n",
    "            # Now that we've identified what the url ending should be for the unique company we're researching, we can now navigate to the corresponding LI search query results\n",
    "\n",
    "            self.browser.get(company_url)\n",
    "            self.time.sleep(4)\n",
    "\n",
    "            # Company Linkedin Page URL\n",
    "            company_li_url = self.browser.find_element(By.XPATH,'//div[@class=\"entity-result__item\"]//a').get_attribute('href')\n",
    "            self.browser.implicitly_wait(4)\n",
    "            self.browser.get(company_li_url)\n",
    "            self.time.sleep(2)\n",
    "\n",
    "            data = []\n",
    "            for search_term in self.search_terms:\n",
    "                results = self.get_search(company, search_term, company_li_url)\n",
    "                data.append(results)\n",
=======
    "    def go_to_page(self, company_url):\n",
    "        self.browser.get(company_url)\n",
    "        self.browser.implicitly_wait(2)\n",
    "\n",
    "        data = []\n",
    "        for search_term in self.search_terms:\n",
    "            results = self.get_search(company_url, search_term)\n",
    "            data.append(results)\n",
    "            \n",
    "        marketing=data[0]\n",
    "        strategy=data[1]\n",
    "        operations=data[2]\n",
    "\n",
    "        row = [company_url, marketing, strategy, operations]\n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "\n",
    "            vp=data[0]\n",
    "            director=data[1]\n",
    "            manager=data[2]\n",
    "            row = [company, vp, director, manager]\n",
    "\n",
<<<<<<< HEAD
    "            with open(\"final_list.csv\", 'a') as csvfile: \n",
    "                object = csv.writer(csvfile)\n",
    "                object.writerow(row)\n",
    "                csvfile.close()\n",
    "        except:\n",
    "            vp=[]\n",
    "            director=[]\n",
    "            manager=[]\n",
    "            row = [company, vp, director, manager]\n",
    "\n",
    "            with open(\"final_list.csv\", 'a') as csvfile: \n",
    "                object = csv.writer(csvfile)\n",
    "                object.writerow(row)\n",
    "                csvfile.close()\n",
    "                \n",
    "    def get_search(self, company, search_query, company_li_url): \n",
    "        #Navigate to company homepage by clicking on search query link. \n",
    "        #Use direct selenium to grab company employee search query page \n",
    "        try:\n",
=======
    "    def get_search(self, company, search_query):\n",
    "        self.time.sleep(2) \n",
    "        #Navigate to company homepage by clicking on search query link. \n",
    "        #Use direct selenium to grab company employee search query page \n",
    "\n",
    "        try: \n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "            company_search_query_page_parent_element = browser.find_element(\n",
    "                By.CLASS_NAME, 'display-flex.mt2.mb1')\n",
    "            company_search_query_page = company_search_query_page_parent_element.find_element(By.CLASS_NAME,\n",
    "            'ember-view')\n",
    "            company_search_query_page.click()\n",
<<<<<<< HEAD
    "            self.time.sleep(4)\n",
=======
    "            self.time.sleep(3)\n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "            search_query_url = str(self.browser.current_url)\n",
    "            position = search_query_url.find('COMPANY_PAGE')\n",
    "            base_emp_search_url = search_query_url[:position]\n",
    "            route = f'{base_emp_search_url}FACETED_SEARCH&title={search_query}'\n",
    "            self.browser.get(route)\n",
<<<<<<< HEAD
    "            self.time.sleep(4)\n",
    "            employees = self.collect(company, search_query, company_li_url)\n",
    "            return employees\n",
    "        except NoSuchElementException: \n",
    "            pass \n",
=======
    "            self.time.sleep(3)\n",
    "            employees = self.collect(company)\n",
    "        except NoSuchElementException: \n",
    "            pass          \n",
    "        \n",
    "        return employees\n",
    "        \n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "\n",
    "    def collect(self, company):\n",
    "        self.time.sleep(3)\n",
    "        try:\n",
    "            # Load page content using beautiful soup\n",
    "            src = browser.page_source\n",
    "            soup = BeautifulSoup(src)\n",
    "            ul_tag = soup.find(\n",
    "                'ul', {'class': 'reusable-search__entity-result-list list-style-none'})\n",
    "            li_tags = ul_tag.find_all('li')\n",
    "            # Loop over each list item and collect data\n",
    "            employees = []\n",
    "            for li_tag in li_tags:\n",
    "                # [Company, Search Query, Name, Title, Location, Employee ]\n",
    "                # Find name and profile link ( Same element )\n",
    "                name_parent = li_tag.find(\n",
    "                    'span', {'class', 'entity-result__title-text t-16'})\n",
    "                target_element = name_parent.find(\"a\", href=True)\n",
    "                # name = target_element.get_text().strip()\n",
    "                page_link = target_element['href']\n",
    "                \n",
    "                # Find title\n",
    "                title = li_tag.find(\n",
    "                    'div', {'class': 'entity-result__primary-subtitle t-14 t-black t-normal'}).get_text().strip()\n",
    "\n",
    "                # Find location\n",
    "                # location = li_tag.find(\n",
    "                #     'div', {'class': 'entity-result__secondary-subtitle t-14 t-normal'}).get_text().strip()\n",
    "                \n",
    "                employee = {\n",
    "                    'title': title,\n",
    "                    'page_link': page_link, \n",
    "                }\n",
    "                employees.append(employee)\n",
    "\n",
    "            # print(employees)\n",
    "            browser.get(company)\n",
    "            self.time.sleep(2)            \n",
    "            return employees\n",
    "\n",
    "        except:\n",
    "            employees = []\n",
    "            # Navigate back to company homepage so new get search can run\n",
    "            browser.get(company_li_url)\n",
    "            self.time.sleep(2)\n",
    "            return employees\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#Login\n",
    "search = Search(browser, search_terms, time)\n",
<<<<<<< HEAD
    "for company in companies: \n",
    "    search.go_to_page(company)\n",
    "    \n",
=======
    "for company_url in companies: \n",
    "    search.go_to_page(company_url)\n",
    " \n",
>>>>>>> 24e4748e90dccd10ca9b0aad1539502161366c91
    "browser.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87c16c7caa655f4bd718a1e1e8093e4b5a751993eaa22ba7f44bf837d2151f7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
